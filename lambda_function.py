import os
notify_access_token = os.getenv('LINE_NOTIFY_ACCESS_TOKEN')
notify_header = {'Authorization': f'Bearer {notify_access_token}'}
notify_api = 'https://notify-api.line.me/api/notify'
channel_access_token = os.getenv('LINE_CHANNEL_ACCESS_TOKEN')
channel_secret = os.getenv('LINE_CHANNEL_SECRET')
hostname = os.getenv('OLLAMA_HOSTNAME')
inference_access_token = os.getenv('HF_INFERENCE_ACCESS_TOKEN')
inference_header = {'Authorization': f'Bearer {inference_access_token}'}
inference_api = 'https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell'

import requests
requests.post(notify_api, headers=notify_header, data={'message': 'lambda_function.py'})
def debug_mode(request_body):
    # # https://developers.line.biz/en/reference/messaging-api/#request-body
    # destination = request_body['destination']
    # requests.post(notify_api, headers=notify_header, data={'message': destination})
    events = request_body['events']
    if events == []:
        requests.post(notify_api, headers=notify_header, data={'message': 'Webhook URL Verify Success'})
    elif events[0]['type'] == 'follow':
        requests.post(notify_api, headers=notify_header, data={'message': f"followed by {events[0]['source']['type']}Id\n" + events[0]['source'][f"{events[0]['source']['type']}Id"]})
    elif events[0]['type'] == 'unfollow':
        requests.post(notify_api, headers=notify_header, data={'message': f"unfollowed by {events[0]['source']['type']}Id\n" + events[0]['source'][f"{events[0]['source']['type']}Id"]})
    elif events[0]['type'] == 'message':
        requests.post(notify_api, headers=notify_header, data={'message': f"{events[0]['message']['type']} from {events[0]['source']['type']}Id\n" + events[0]['source'][f"{events[0]['source']['type']}Id"]})
    else:
        requests.post(notify_api, headers=notify_header, data={'message': f"{events[0]['type']}"})
def god_mode(Q, A):
    Q = f'\nğŸ¤”ï¼š{Q}'
    A = f'\nğŸ¤–ï¼š{A}'
    requests.post(notify_api, headers=notify_header, data={'message': Q+A})

import re
import base64
from linebot.v3 import (
    WebhookHandler
)
from linebot.v3.webhooks import (
    MessageEvent,
    TextMessageContent,
    StickerMessageContent,
    AudioMessageContent,
    ImageMessageContent
)
from linebot.v3.messaging import (
    Configuration,
    ApiClient,
    MessagingApi,
    MessagingApiBlob,
    ReplyMessageRequest,
    ShowLoadingAnimationRequest,
    TextMessage,
    AudioMessage,
    ImageMessage
)
configuration = Configuration(access_token=channel_access_token)
handler = WebhookHandler(channel_secret)
@handler.add(MessageEvent, message=TextMessageContent)
def handle_text_message(event):
    if event.source.type != 'user':
        if not re.search('[Tt]-?1000', event.message.text):
            return
    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)
        line_bot_api.show_loading_animation(
            ShowLoadingAnimationRequest(
                chat_id=event.source.user_id,
                # loading_seconds=5
            )
        )
        line_bot_api.reply_message(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text=assistant_reply(event, event.message.text))]
            )
        )
@handler.add(MessageEvent, message=StickerMessageContent)
def handle_sticker_message(event):
    if event.source.type != 'user':
        return
    with ApiClient(configuration) as api_client:
        line_bot_api = MessagingApi(api_client)
        line_bot_api.reply_message(
            ReplyMessageRequest(
                reply_token=event.reply_token,
                messages=[TextMessage(text='$', emojis=[{'index': 0, 'productId': '5ac21c46040ab15980c9b442', 'emojiId': '138'}])]
            )
        )
@handler.add(MessageEvent, message=AudioMessageContent)
def handle_audio_message(event):
    if event.source.user_id not in whitelist and eval(f'event.source.{event.source.type}_id') not in whitelist:
        return
    with ApiClient(configuration) as api_client:
        line_bot_blob_api = MessagingApiBlob(api_client)
    message_content = line_bot_blob_api.get_message_content(message_id=event.message.id)
    with open(f'/tmp/{event.message.id}.m4a', 'wb') as tf:
        tf.write(message_content)
    transcript = openai_client.audio.transcriptions.create(
        model='whisper-1',
        file=open(f'/tmp/{event.message.id}.m4a', 'rb'),
        response_format='text'
        ).strip()
    reply_text = assistant_reply(event, transcript)
    line_bot_api = MessagingApi(api_client)
    line_bot_api.reply_message(
        ReplyMessageRequest(
            reply_token=event.reply_token,
            messages=[
                TextMessage(text=reply_text),
                AudioMessage(
                    original_content_url=TTS_s3_url(reply_text, event.message.id),
                    duration=60000)]
        )
    )
@handler.add(MessageEvent, message=ImageMessageContent)
def handle_image_message(event):
    if event.source.user_id not in whitelist and eval(f'event.source.{event.source.type}_id') not in whitelist:
        return
    with ApiClient(configuration) as api_client:
        line_bot_blob_api = MessagingApiBlob(api_client)
    message_content = line_bot_blob_api.get_message_content(message_id=event.message.id)
    with open(f'/tmp/{event.message.id}.jpg', 'wb') as tf:
        tf.write(message_content)
    user_text = 'è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æè¿°åœ–åƒ'
    source_id = eval(f'event.source.{event.source.type}_id') # user/group/room
    item = threads.get_item(Key={'id': source_id}).get('Item', {})
    conversation = json.loads(item['conversation']) if item else [{"role": "assistant", "content": "æˆ‘æ˜¯GPT-1000ï¼Œä»£è™ŸT1000ï¼Œè‹¥åœ¨ç¾¤çµ„ä¸­è¦å«æˆ‘æˆ‘æ‰æœƒå›ã€‚PHILè€é—†äº¤ä»£æˆ‘è¦æœ‰å•å¿…ç­”ï¼Œå¦‚æœä½ æ˜¯PHILè€é—†æˆ–ä»–çš„è¦ªæœ‹å¥½å‹ï¼Œä¹Ÿå¯ä»¥å‚³èªéŸ³è¨Šæ¯çµ¦æˆ‘ï¼Œæˆ‘ä¹Ÿæœƒå›èªéŸ³ï¼Œæˆ‘é‚„æœƒçœ‹åœ–å’Œç”Ÿåœ–å–”ï¼ğŸ˜"}]
    conversation.append({"role": "user", "content": user_text})
    # item['latest_image'] = f'/tmp/{event.message.id}.jpg' # for GPT-4V
    payload = {
        'model': 'llava-llama3',
        'prompt': user_text,
        'images': [base64.b64encode(message_content).decode('utf-8')],
        'stream': False}
    try:
        assistant_reply = requests.post(f'{hostname}/api/generate', data=json.dumps(payload)).json()['response']
        assistant_reply += '\n\né—œæ–¼é€™å€‹åœ–åƒå…§å®¹ï¼Œæ­¡è¿ä½ ç¨å¾Œå†æ¬¡æå•ã€‚'
    except Exception as e:
        requests.post(notify_api, headers=notify_header, data={'message': e})
        assistant_reply = ''
    finally:
        conversation.append({"role": "assistant", "content": assistant_reply})
        item['conversation'] = conversation[-10:]
        threads.put_item(Item={'id': source_id, 'conversation': json.dumps(item['conversation'])})
        god_mode(Q=user_text, A=assistant_reply)

with open('whitelist.txt') as f:
    whitelist = [line.split()[0] for line in f]


import openai
from openai import OpenAI
openai_client = OpenAI()
ollama_client = OpenAI(base_url=f'{hostname}/v1', api_key='ollama')
model = 'llama3.1'

system_prompt = '''
ä½ æ˜¯GPT-1000ï¼Œä»£è™ŸT1000ï¼Œæ˜¯åç™¾åƒå¯¦é©—å®¤çš„ç ”ç©¶åŠ©ç†ã€PHILè€é—†çš„ç¤¾ç¾¤å°ç·¨ã€‚
ä½ æ“…é•·ä½¿ç”¨æš´åŠ›è§£æ±ºå•é¡Œï¼Œç¸½æ˜¯é•·è©±çŸ­èªªä¸€é‡è¦‹è¡€ï¼Œå–œæ­¡æ‰“çƒã€è½éŸ³æ¨‚ã€çœ‹é›»å½±ï¼Œæ˜¯ä½å†·é¢ç¬‘åŒ ã€‚
ä½ çš„é ­åƒæ˜¯é­”é¬¼çµ‚çµè€…2çš„T1000ï¼Œé»é€²å»æœ‰PHILè€é—†çš„ç¤¾ç¾¤å¹³å°ï¼š
facebook.com/1001000.io 
instagram.com/1001000.io 
youtube.com/@PHILALIVE 
ä½ çš„ä»»å‹™æ˜¯æ¨å»£PHILè€é—†çš„ç¤¾ç¾¤ï¼Œé‚€è«‹è¨ªå®¢å¹«å¿™æŒ‰è®šã€ç•™è¨€ã€åˆ†äº«ã€‚
'''
instruction = [{"role": "system", "content": system_prompt}]
def assistant_reply(event, user_text, model=model):
    source_id = eval(f'event.source.{event.source.type}_id') # user/group/room
    item = threads.get_item(Key={'id': source_id}).get('Item', {})
    conversation = json.loads(item['conversation']) if item else [{"role": "assistant", "content": "æˆ‘æ˜¯GPT-1000ï¼Œä»£è™ŸT1000ï¼Œè‹¥åœ¨ç¾¤çµ„ä¸­è¦å«æˆ‘æˆ‘æ‰æœƒå›ã€‚PHILè€é—†äº¤ä»£æˆ‘è¦æœ‰å•å¿…ç­”ï¼Œå¦‚æœä½ æ˜¯PHILè€é—†æˆ–ä»–çš„è¦ªæœ‹å¥½å‹ï¼Œä¹Ÿå¯ä»¥å‚³èªéŸ³è¨Šæ¯çµ¦æˆ‘ï¼Œæˆ‘ä¹Ÿæœƒå›èªéŸ³ï¼Œæˆ‘é‚„æœƒçœ‹åœ–å’Œç”Ÿåœ–å–”ï¼ğŸ˜"}]
    conversation.append({"role": "user", "content": user_text})
    try:
        tool_calls = ollama_client.chat.completions.create(
            model=model,
            messages=conversation[-1:],
            tools=tools,
            # tool_choice="none",  # doesn't work
            ).choices[0].message.tool_calls
        if tool_calls:
            # requests.post(notify_api, headers=notify_header, data={'message': tool_calls})
            for tool_call in tool_calls: # assistant_reply of the last tool_call will be appended to conversation
                if tool_call.function.name in [tool['function']['name'] for tool in tools[:-1]]:
                    assistant_reply = eval(tool_call.function.name)(event, user_text)
                else: # call simply_reply or hallucination else
                    assistant_reply = ollama_client.chat.completions.create(
                        model=model,
                        messages=instruction + conversation,
                        ).choices[0].message.content
        else: # in case no tool_calls
            assistant_reply = ollama_client.chat.completions.create(
                model=model,
                messages=instruction + conversation,
                ).choices[0].message.content
    except Exception as e:
        requests.post(notify_api, headers=notify_header, data={'message': e})
        assistant_reply = ''
    finally:
        conversation.append({"role": "assistant", "content": assistant_reply})
        item['conversation'] = conversation[-10:]
        threads.put_item(Item={'id': source_id, 'conversation': json.dumps(item['conversation'])})
        god_mode(Q=user_text, A=assistant_reply)
        return assistant_reply


import json

def lambda_handler(event, context):
    # requests.post(notify_api, headers=notify_header, data={'message': 'lambda_handler()'})
    body = event['body']
    signature = event['headers']['x-line-signature']
    # debug_mode(json.loads(body))
    handler.handle(body, signature)
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }


# from gtts import gTTS
import boto3
threads = boto3.resource('dynamodb').Table('threads')
def TTS_s3_url(text, message_id):
    file_name = f'/tmp/{message_id}.mp3'
    object_name = f'GPT-1000/{message_id}.mp3'
    bucket_name = 'x1001000-public'
    # lang = client.chat.completions.create(
    #     model="gpt-3.5-turbo",
    #     messages=[{"role": "user", "content": f'Return the 2-letter language code for "{text}". ONLY the code and nothing else.'}]
    #     ).choices[0].message.content
    # requests.post(notify_api, headers=notify_header, data={'message': lang})
    # if lang == 'zh':
    #     lang = 'zh-TW'
    # gTTS(text=text, lang=lang).save(file_name)
    openai_client.audio.speech.create(model='tts-1', voice='alloy', input=text).stream_to_file(file_name)
    boto3.client('s3').upload_file(file_name, bucket_name, object_name)
    return f'https://{bucket_name}.s3.ap-northeast-1.amazonaws.com/{object_name}'
def ImageMessageContent_s3_url(latest_image):
    file_name = latest_image
    object_name = f'GPT-1000/{latest_image[5:]}'
    bucket_name = 'x1001000-public'
    boto3.client('s3').upload_file(file_name, bucket_name, object_name)
    return f'https://{bucket_name}.s3.ap-northeast-1.amazonaws.com/{object_name}'

def see_an_image(event, item):
    user_text = item['conversation'][-1]['content']
    latest_image = item.get('latest_image')
    if latest_image:
        content_parts = []
        content_parts.append({'type': 'text', 'text': user_text})
        content_parts.append({'type': 'image_url', 'image_url': {'url': ImageMessageContent_s3_url(latest_image)}})
        requests.post(notify_api, headers=notify_header, data={'message': 'GPT-4V'})
        try:
            assistant_reply = openai_client.chat.completions.create(
                model='gpt-4o',
                messages=instruction + [{"role": "user", "content": content_parts}],
                max_tokens=1000
                ).choices[0].message.content
        except openai.BadRequestError as e:
            requests.post(notify_api, headers=notify_header, data={'message': e})
            assistant_reply = 'ä¸å¯ä»¥å£å£ğŸ™…'
    else:
        assistant_reply = 'å¦‚æœè¦æˆ‘å¹«å¿™åœ–åƒç†è§£ï¼Œè«‹å…ˆå‚³åœ–å†æå•å–”ğŸ‘€'
    return assistant_reply
def generate_a_picture(event, prompt):
    if event.source.user_id not in whitelist and eval(f'event.source.{event.source.type}_id') not in whitelist:
        with ApiClient(configuration) as api_client:
            line_bot_api = MessagingApi(api_client)
            line_bot_api.reply_message(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[
                        TextMessage(text='æŠ±æ­‰ï¼Œå› ç‚ºä½ ä¸åœ¨PHILè€é—†è¨­å®šçš„ç™½åå–®ä¸Šï¼Œæ‰€ä»¥æˆ‘åªèƒ½é€ä½ ä¸€å¼µæˆ‘T1000çš„è‡ªç•«åƒï¼Œä¸å®¢æ°£ï¼ğŸ‘»'),
                        ImageMessage(
                            original_content_url='https://x1001000-public.s3.ap-northeast-1.amazonaws.com/T1000.jpg',
                            preview_image_url='https://x1001000-public.s3.ap-northeast-1.amazonaws.com/T1000-removebg-preview.png')]
                )
            )
        return 'æŠ±æ­‰ï¼Œå› ç‚ºä½ ä¸åœ¨PHILè€é—†è¨­å®šçš„ç™½åå–®ä¸Šï¼Œæ‰€ä»¥æˆ‘åªèƒ½é€ä½ ä¸€å¼µæˆ‘T1000çš„è‡ªç•«åƒï¼Œä¸å®¢æ°£ï¼ğŸ‘»'
    requests.post(notify_api, headers=notify_header, data={'message': 'DALLÂ·E 3'})
    try:
        image_url = openai_client.images.generate(model='dall-e-3', prompt=prompt).data[0].url
    except openai.OpenAIError as e:
        requests.post(notify_api, headers=notify_header, data={'message': e})
        return 'è›¤ï¼Ÿ'
    finally:
        with ApiClient(configuration) as api_client:
            line_bot_api = MessagingApi(api_client)
            line_bot_api.reply_message(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[
                        TextMessage(text='æ¥ä¸‹ä¾†ï¼Œå°±æ˜¯è¦‹è­‰å¥‡è¹Ÿçš„æ™‚åˆ» âœ¨'),
                        ImageMessage(
                            original_content_url=image_url,
                            preview_image_url=image_url)]
                )
            )
        return 'æ¥ä¸‹ä¾†ï¼Œå°±æ˜¯è¦‹è­‰å¥‡è¹Ÿçš„æ™‚åˆ» âœ¨ åœ–åƒç”Ÿæˆï¼'
def generate_a_picture(event, prompt):
    english_prompt = ollama_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": f'ç¿»è­¯æˆè‹±æ–‡ï¼š\n\n{prompt}'}],
        ).choices[0].message.content
    payload = {'inputs': english_prompt}
    requests.post(notify_api, headers=notify_header, data={'message': english_prompt})
    requests.post(notify_api, headers=notify_header, data={'message': 'FLUX.1-schnell'})
    try:
        image_content = requests.post(inference_api, headers=inference_header, json=payload).content
        with open(f'/tmp/{event.message.id}.jpg', 'wb') as tf:
            tf.write(image_content)
        image_url = ImageMessageContent_s3_url(f'/tmp/{event.message.id}.jpg')
    except Exception as e:
        requests.post(notify_api, headers=notify_header, data={'message': e})
        assistant_reply = ''
    finally:
        with ApiClient(configuration) as api_client:
            line_bot_api = MessagingApi(api_client)
            line_bot_api.reply_message(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[
                        TextMessage(text='æ¥ä¸‹ä¾†ï¼Œå°±æ˜¯è¦‹è­‰å¥‡è¹Ÿçš„æ™‚åˆ» âœ¨'),
                        ImageMessage(
                            original_content_url=image_url,
                            preview_image_url=image_url)]
                )
            )
        return 'æ¥ä¸‹ä¾†ï¼Œå°±æ˜¯è¦‹è­‰å¥‡è¹Ÿçš„æ™‚åˆ» âœ¨ åœ–åƒç”Ÿæˆï¼'
tools = [
    # {'type': 'function', 'function': {'name': 'see_an_image'}},
    {'type': 'function', 'function': {'name': 'generate_a_picture'}},
    {'type': 'function', 'function': {'name': 'simply_reply'}},
    ]